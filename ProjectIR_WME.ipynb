{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Useful commands for installing missing dependencies (to be used on jupyter notebook)\n",
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} gensim\n",
    "!conda install --yes --prefix {sys.prefix} --no-update-dependencies -c conda-forge pulp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pulp\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Word2Vec model (https://code.google.com/archive/p/word2vec/). The next block may require a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\User\\Desktop\\projectIR\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Word2Vec model (@Mario: Nota quanto cazzo Ã¨ incredibile sta cosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = wvmodel['man']+wvmodel['king']-wvmodel['woman']\n",
    "y_test = wvmodel['queen']\n",
    "z_test = wvmodel['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4057604]]\n",
      "[[0.01503608]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([x_test], [y_test]))\n",
    "print(cosine_similarity([x_test], [z_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the collection&nbsp;. https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html &nbsp;\n",
    "We use the smaller dataset (called mini). We tokenize all documents and assign to every document an unique ID and a label (depending on the group it came from). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do, parse all documents and get the list of tokens (We remove capital letters?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Word Mover Distance (solved through LP). Next two blocks from: https://github.com/stephenhky/PyWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs = {token: wvmodel[token] for token in all_tokens}\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WMD_regular(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define WMD also for a random document with random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance_random(documents_token, random_document_vectors, wvmodel, lpFile=None):\n",
    "    #We create a random association between the random_document_vectors and tokens, assuming random vectors to be all different\n",
    "    first_sent_tokens = documents_token\n",
    "    second_sent_tokens = ['__qxvca^&3fd?#_!$' +  str(i) for i in range(len(random_document_vectors))]\n",
    "    \n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs_document = {token: wvmodel[token] for token in first_sent_tokens}\n",
    "    wordvecs_random = {second_sent_tokens[i]: random_document_vectors[i] for i in range(len(random_document_vectors))}\n",
    "    #print(wordvecs_random)\n",
    "    wordvecs = merge_two_dicts(wordvecs_document,wordvecs_random)\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WMD_random(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_random(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test WMD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6146241426467896\n",
      "3.6146241426467896\n"
     ]
    }
   ],
   "source": [
    "print(WMD_regular(['hi','bye'],['see','you'],wvmodel))\n",
    "print(WMD_random(['hi','bye'],[wvmodel['see'],wvmodel['you']],wvmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementation of the WME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WME_phi(x,w,gamma,wvmodel):\n",
    "    return np.exp( -gamma*WMD_random(x,w,wvmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-99-3d0e25626e1d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-99-3d0e25626e1d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    # It must returns a list of text embeddings, i-th element being the embedding of the i-th document\u001b[0m\n\u001b[1;37m                                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def calculate_WME(documents, D_max, R, wvmodel):\n",
    "    # It must returns a list of text embeddings, i-th element being the embedding of the i-th document\n",
    "    \n",
    "    #Phase 1: Compute v_max and v_min\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset import. RECIPE dataset has been used, in json format. The training set contains, for all the recipts, the belonging cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774\n",
      "greek\n",
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_json_data = open('recipe_dataset/train.json')\n",
    "test_json_data = open('recipe_dataset/test.json')\n",
    "train_data = json.load(train_json_data)\n",
    "test_data = json.load(test_json_data)\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[0]['cuisine'])\n",
    "print(train_data[0]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
