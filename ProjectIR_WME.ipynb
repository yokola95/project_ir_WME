{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Useful commands for installing missing dependencies (to be used on jupyter notebook)\n",
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} gensim\n",
    "!conda install --yes --prefix {sys.prefix} --no-update-dependencies -c conda-forge pulp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cosine\n",
    "import pulp\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Word2Vec model (https://code.google.com/archive/p/word2vec/). The next block may require a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Word2Vec model (@Mario: Nota quanto cazzo Ã¨ incredibile sta cosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = wvmodel['man']+wvmodel['king']-wvmodel['woman']\n",
    "y_test = wvmodel['queen']\n",
    "z_test = wvmodel['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40576041]]\n",
      "[[ 0.01503608]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([x_test], [y_test]))\n",
    "print(cosine_similarity([x_test], [z_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product_lists(a,b):\n",
    "    return sum([x*y for x,y in zip(a,b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wme_to_wmd(x,y,gamma):\n",
    "    kxy = dot_product_lists(x,y)\n",
    "    return -1/gamma*np.log(kxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Word Mover Distance (solved through LP). Next two blocks from: https://github.com/stephenhky/PyWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_fracdict(tokens):\n",
    "    cntdict = defaultdict(lambda : 0)\n",
    "    for token in tokens:\n",
    "        cntdict[token] += 1\n",
    "    totalcnt = sum(cntdict.values())\n",
    "    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UBER MEGA FAST WMD BY AL\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "def wmdistance(document1, document2, wvmodel):\n",
    "        from pyemd import emd\n",
    "\n",
    "        # Remove out-of-vocabulary words.\n",
    "        len_pre_oov1 = len(document1)\n",
    "        len_pre_oov2 = len(document2)\n",
    "        remember_me = document2\n",
    "        document2 = ['__qxvca^&3fd?#_!$' +  str(i) for i in range(len(document2))]\n",
    "        diff1 = len_pre_oov1 - len(document1)\n",
    "        diff2 = len_pre_oov2 - len(document2)\n",
    "        if diff1 > 0 or diff2 > 0:\n",
    "            logger.info('Removed %d and %d OOV words from document 1 and 2 (respectively).', diff1, diff2)\n",
    "\n",
    "        if not document1 or not document2:\n",
    "            logger.info(\n",
    "                \"At least one of the documents had no words that were in the vocabulary. \"\n",
    "                \"Aborting (returning inf).\"\n",
    "            )\n",
    "            return float('inf')\n",
    "\n",
    "        dictionary = Dictionary(documents=[document1, document2])\n",
    "        vocab_len = len(dictionary)\n",
    "\n",
    "        if vocab_len == 1:\n",
    "            # Both documents are composed by a single unique token\n",
    "            return 0.0\n",
    "\n",
    "        # Sets for faster look-up.\n",
    "        docset1 = set(document1)\n",
    "        docset2 = set(document2)\n",
    "\n",
    "        wordvecs_random = {document2[i]: remember_me[i] for i in range(len(remember_me))}\n",
    "\n",
    "        # Compute distance matrix.\n",
    "        distance_matrix = np.zeros((vocab_len, vocab_len))\n",
    "        for i, t1 in dictionary.items():\n",
    "            if t1 not in docset1:\n",
    "                continue\n",
    "\n",
    "            for j, t2 in dictionary.items():\n",
    "                if t2 not in docset2 or distance_matrix[i, j] != 0.0:\n",
    "                    continue\n",
    "\n",
    "                # Compute Euclidean distance between word vectors.\n",
    "                distance_matrix[i, j] = distance_matrix[j, i] = np.sqrt(sum((wvmodel[t1] - wordvecs_random[t2])**2))\n",
    "\n",
    "        def nbow(document):\n",
    "            d = np.zeros(vocab_len)\n",
    "            nbow = dictionary.doc2bow(document)  # Word frequencies.\n",
    "            doc_len = len(document)\n",
    "            for idx, freq in nbow:\n",
    "                d[idx] = freq / float(doc_len)  # Normalized word frequencies.\n",
    "            return d\n",
    "\n",
    "        # Compute nBOW representation of documents.\n",
    "        d1 = nbow(document1)\n",
    "        d2 = nbow(document2)\n",
    "\n",
    "        # Compute WMD.\n",
    "        return emd(d1, d2, distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs = {token: wvmodel[token] for token in all_tokens}\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WMD_regular(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_probspec(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define WMD also for a random document with random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_mover_distance_random(documents_token, random_document_vectors, wvmodel, lpFile=None):\n",
    "    #We create a random association between the random_document_vectors and tokens, assuming random vectors to be all different\n",
    "    first_sent_tokens = documents_token\n",
    "    second_sent_tokens = ['__qxvca^&3fd?#_!$' +  str(i) for i in range(len(random_document_vectors))]\n",
    "    \n",
    "    all_tokens = list(set(first_sent_tokens+second_sent_tokens))\n",
    "    wordvecs_document = {token: wvmodel[token] for token in first_sent_tokens}\n",
    "    wordvecs_random = {second_sent_tokens[i]: random_document_vectors[i] for i in range(len(random_document_vectors))}\n",
    "    #print(wordvecs_random)\n",
    "    wordvecs = merge_two_dicts(wordvecs_document,wordvecs_random)\n",
    "\n",
    "    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n",
    "    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n",
    "\n",
    "    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n",
    "\n",
    "    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n",
    "    prob += pulp.lpSum([T[token1, token2]*euclidean(wordvecs[token1], wordvecs[token2])\n",
    "                        for token1, token2 in product(all_tokens, all_tokens)])\n",
    "    for token2 in second_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets])==second_sent_buckets[token2]\n",
    "    for token1 in first_sent_buckets:\n",
    "        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets])==first_sent_buckets[token1]\n",
    "\n",
    "    if lpFile!=None:\n",
    "        prob.writeLP(lpFile)\n",
    "\n",
    "    prob.solve()\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WMD_random(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=None):\n",
    "    prob = word_mover_distance_random(first_sent_tokens, second_sent_tokens, wvmodel, lpFile=lpFile)\n",
    "    return pulp.value(prob.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test WMD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.61462414265\n",
      "3.61462414265\n",
      "3.61462488407\n"
     ]
    }
   ],
   "source": [
    "print(WMD_regular(['hi','bye'],['see','you'],wvmodel))\n",
    "print(WMD_random(['hi','bye'],[wvmodel['see'],wvmodel['you']],wvmodel))\n",
    "print(wmdistance(['hi','bye'],wvmodel['see','you'],wvmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementation of the WME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WME_phi(x,w,gamma,wvmodel):\n",
    "    return np.exp( -gamma*wmdistance(x,w,wvmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_WME(documents, D_max, R, wvmodel, gamma):\n",
    "    # It must returns a list of text embeddings, i-th element being the embedding of the i-th document\n",
    "    \n",
    "    #Phase 1: Compute v_max and v_min\n",
    "    v_values = [+9999,-9999] # [v_min,v_max]\n",
    "    for doc in documents:\n",
    "        for token in doc:\n",
    "            for x in wvmodel[token]:\n",
    "                v_values[0] = min(v_values[0],x)\n",
    "                v_values[1] = max(v_values[1],x)\n",
    "            \n",
    "    print(\"[v_min, v_max] = \"  + str(v_values))\n",
    "    Z = []\n",
    "    for j in range(R):\n",
    "        print(\"R: \" + str(j+1)+\"/\"+str(R))\n",
    "        D = 1 + np.random.randint(D_max)\n",
    "        random_doc = []\n",
    "        for l in range(D):\n",
    "            word = np.random.uniform(v_values[0],v_values[1],size=300)\n",
    "            word = word*1/np.sqrt(sum(x*x for x in word))\n",
    "            random_doc.append(word)\n",
    "        to_add = [WME_phi(doc,random_doc,gamma,wvmodel) for doc in documents]\n",
    "        #print(\"to_add\" + str(to_add))\n",
    "        Z.append(to_add)\n",
    "    ret_Z = (1/(np.sqrt(R))*np.array(Z).T).tolist()\n",
    "    return ret_Z\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [['the','sun','is','the','best','thing','in','the','world'],['i','love','watch','the','sun','it','burns','my','life']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It returns a list of k_max elements, iteratively picked using the farthest-first traversal algorithm\n",
    "# The returned list [a_1, ..., a_(k_max)] containts the indexes of the picked element at every iteration (i.e. a_i is selected in the i-th iteration)\n",
    "# documents in this case containts the (ordered) list of the embeddings of the documents\n",
    "def k_center(documents,k_max):\n",
    "    ret = []\n",
    "    radius = []\n",
    "    N = len(documents)\n",
    "    first = np.random.randint(N)\n",
    "    ret.append(first)\n",
    "    \n",
    "    dist = [euclidean(doc,documents[first]) for doc in documents] \n",
    "   \n",
    "    radius.append(np.max(dist))\n",
    "    for k in range(k_max-1):\n",
    "        #Selection\n",
    "        j = np.argmax(dist)\n",
    "        ret.append(j)\n",
    "        \n",
    "        #Update\n",
    "        dist = [min(dist[i], euclidean(documents[i],documents[j])) for i in range(N)]\n",
    "       \n",
    "        radius.append(np.max(dist))\n",
    "    return ret,radius\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "f = open('newsgroup_dataset.pckl','rb')\n",
    "obj = pickle.load(f)\n",
    "f.close()\n",
    "name_dataset = obj[0]\n",
    "train_R_X = obj[1]\n",
    "train_R_Y = obj[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARE! The next line of code is computationally expensive! (Disabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "train_embedding = calculate_WME(train_R_X,6,128,wvmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block can be used to save the trained model (Put it as code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle \n",
    "obj = ['newsgroup_WME-D6-R128-N1500-L25', train_R_X, train_R_Y, train_embedding] \n",
    "f = open('newsgroup_WME-D6-R128-N1500-L25.pckl', 'wb') \n",
    "pickle.dump(obj, f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "obj = ['recipe_dataset', train_R_X, train_R_Y, index_to_pick, train_embedding]\n",
    "f = open('recipes_WME.pckl', 'wb')\n",
    "pickle.dump(obj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block can be used to load the trained model (Put it as code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "f = open('newsgroup_WME-D6-R128-N1500-L25.pckl', 'rb')\n",
    "obj = pickle.load(f)\n",
    "f.close()\n",
    "print(len(obj))\n",
    "name_dataset = obj[0]\n",
    "train_R_X = obj[1]\n",
    "train_R_Y = obj[2]\n",
    "train_embedding = obj[3]\n",
    "print(name_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elements_to_pick = 150\n",
    "NUMBER_OF_POINTS = len(train_R_X)\n",
    "centers,radius = k_center(train_embedding,elements_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets_label = set()\n",
    "k_center_number = []\n",
    "for i in range(elements_to_pick):\n",
    "    sets_label.add(train_R_Y[centers[i]])\n",
    "    k_center_number.append(len(sets_label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_indexes = [i for i in range(NUMBER_OF_POINTS)]\n",
    "np.random.shuffle(random_indexes)\n",
    "sets_label = set()\n",
    "random_number = [0]*elements_to_pick\n",
    "NUMBER_ITERATIONS = 100000 #Reduce if too much expensive\n",
    "for j in range(NUMBER_ITERATIONS):\n",
    "    sets_label = set()\n",
    "    for i in range(150):\n",
    "        sets_label.add(train_R_Y[random_indexes[i]])\n",
    "        random_number[i] += 1.0/NUMBER_ITERATIONS*len(sets_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ARI(dataset, centers, labels):\n",
    "    dict_labels = {}\n",
    "    ii = 0\n",
    "    for x in set(labels):\n",
    "        if x not in dict_labels:\n",
    "            dict_labels[x] = ii\n",
    "            ii = ii+1\n",
    "    true_labels = [dict_labels[x] for x in labels]\n",
    "    found_labels = []\n",
    "    for x in dataset:\n",
    "        j = np.argmin([euclidean(x,c) for c in centers])\n",
    "        found_labels.append(j)\n",
    "    print(true_labels)\n",
    "    print(found_labels)\n",
    "    return [metrics.adjusted_rand_score(true_labels, found_labels),metrics.adjusted_mutual_info_score(true_labels,found_labels)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tennis' 'football' 'cricket' 'cricket' 'football' 'rugby' 'rugby'\n",
      " 'football' 'athletics' 'cricket' 'athletics' 'cricket' 'tennis'\n",
      " 'athletics' 'rugby' 'tennis' 'rugby' 'athletics' 'tennis' 'cricket'\n",
      " 'athletics' 'cricket' 'athletics' 'cricket' 'cricket' 'tennis' 'cricket'\n",
      " 'football' 'football' 'cricket' 'athletics' 'cricket' 'tennis' 'cricket'\n",
      " 'rugby' 'athletics' 'football' 'cricket' 'athletics' 'tennis' 'athletics'\n",
      " 'cricket' 'football' 'cricket' 'cricket' 'athletics' 'football'\n",
      " 'athletics' 'athletics' 'rugby' 'athletics' 'athletics' 'athletics'\n",
      " 'tennis' 'rugby' 'rugby' 'tennis' 'football' 'tennis' 'rugby' 'athletics'\n",
      " 'rugby' 'cricket' 'cricket' 'athletics' 'rugby' 'athletics' 'athletics'\n",
      " 'football' 'rugby' 'athletics' 'athletics' 'athletics' 'football'\n",
      " 'football' 'cricket' 'rugby' 'tennis' 'athletics' 'tennis' 'cricket'\n",
      " 'football' 'cricket' 'tennis' 'athletics' 'athletics' 'rugby' 'tennis'\n",
      " 'cricket' 'athletics' 'rugby' 'tennis' 'cricket' 'cricket' 'football'\n",
      " 'tennis' 'tennis' 'cricket' 'rugby' 'tennis' 'football' 'cricket' 'rugby'\n",
      " 'cricket' 'athletics' 'athletics' 'rugby' 'football' 'tennis' 'tennis'\n",
      " 'athletics' 'rugby' 'football' 'football' 'athletics' 'rugby' 'football'\n",
      " 'cricket' 'athletics' 'football' 'tennis' 'football' 'rugby' 'cricket'\n",
      " 'football' 'football' 'rugby' 'rugby' 'athletics' 'cricket' 'athletics'\n",
      " 'football' 'rugby' 'tennis' 'football' 'football' 'rugby' 'athletics'\n",
      " 'tennis' 'football' 'cricket' 'football' 'tennis' 'tennis' 'football'\n",
      " 'football' 'tennis' 'tennis' 'football' 'athletics' 'tennis' 'cricket'\n",
      " 'cricket' 'cricket' 'rugby' 'athletics' 'tennis' 'football' 'rugby'\n",
      " 'athletics' 'football' 'football' 'football' 'football' 'rugby' 'tennis'\n",
      " 'tennis' 'tennis' 'rugby' 'athletics' 'football' 'athletics' 'cricket'\n",
      " 'tennis' 'rugby' 'football' 'tennis' 'rugby' 'rugby' 'football' 'cricket'\n",
      " 'cricket' 'athletics' 'football' 'rugby' 'athletics' 'rugby' 'athletics'\n",
      " 'athletics' 'tennis' 'rugby' 'athletics' 'rugby' 'rugby' 'cricket'\n",
      " 'cricket' 'cricket' 'cricket' 'athletics' 'football' 'rugby' 'football'\n",
      " 'cricket' 'rugby' 'rugby' 'tennis' 'football' 'athletics' 'tennis'\n",
      " 'athletics' 'tennis' 'tennis' 'football' 'football' 'rugby' 'athletics'\n",
      " 'tennis' 'athletics' 'football' 'rugby' 'rugby' 'rugby' 'athletics'\n",
      " 'athletics' 'cricket' 'football' 'cricket' 'tennis' 'athletics' 'rugby'\n",
      " 'rugby' 'cricket' 'athletics' 'rugby' 'rugby' 'cricket' 'football' 'rugby'\n",
      " 'athletics' 'cricket' 'athletics' 'rugby' 'cricket' 'tennis' 'rugby'\n",
      " 'rugby' 'rugby' 'rugby' 'tennis' 'rugby' 'rugby' 'football' 'rugby'\n",
      " 'rugby' 'rugby' 'rugby' 'football' 'athletics' 'athletics' 'football'\n",
      " 'football' 'rugby' 'football' 'football' 'tennis' 'tennis' 'tennis'\n",
      " 'tennis' 'athletics' 'tennis' 'tennis' 'cricket' 'cricket' 'tennis'\n",
      " 'athletics' 'athletics' 'athletics' 'tennis' 'football' 'cricket' 'tennis'\n",
      " 'tennis' 'tennis' 'athletics' 'rugby' 'cricket' 'tennis' 'football'\n",
      " 'tennis' 'football' 'football' 'rugby' 'cricket' 'cricket' 'tennis'\n",
      " 'cricket' 'football' 'athletics' 'tennis' 'rugby' 'athletics' 'tennis'\n",
      " 'tennis' 'football' 'athletics' 'football' 'football' 'cricket' 'cricket'\n",
      " 'football' 'athletics' 'athletics' 'rugby' 'tennis' 'athletics' 'tennis'\n",
      " 'football' 'tennis' 'cricket' 'tennis' 'athletics' 'cricket' 'cricket'\n",
      " 'cricket' 'rugby' 'athletics' 'football' 'tennis' 'cricket' 'tennis'\n",
      " 'football' 'football' 'athletics' 'rugby' 'tennis' 'tennis' 'athletics'\n",
      " 'cricket' 'rugby' 'rugby' 'cricket' 'football' 'tennis' 'cricket'\n",
      " 'cricket' 'football' 'cricket' 'cricket' 'athletics' 'football' 'tennis'\n",
      " 'cricket' 'football' 'rugby' 'football' 'tennis' 'cricket' 'cricket'\n",
      " 'football' 'rugby' 'athletics' 'rugby' 'cricket' 'cricket' 'tennis'\n",
      " 'cricket' 'cricket' 'tennis' 'rugby' 'tennis' 'tennis' 'rugby' 'football'\n",
      " 'rugby' 'football']\n"
     ]
    }
   ],
   "source": [
    "print(train_R_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f = open('bbcsport_embeddingsR=512_gamma=1p12', 'wb')\n",
    "obj = ['bbcsport_embeddings', all_train_embeddings, train_R_Y, cutoff, gammas, ] \n",
    "pickle.dump(obj, f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('bbcsport_dataset.pckl','rb')\n",
    "obj = pickle.load(f)\n",
    "f.close()\n",
    "name_dataset = obj[0]\n",
    "train_R_X = obj[1]\n",
    "train_R_Y = obj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./liblinear-2.21/python/')\n",
    "from liblinearutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jj = 0\n",
    "to_number = {}\n",
    "for x in set(train_R_Y):\n",
    "    to_number[x] = jj\n",
    "    jj = jj + 1\n",
    "int_train_R_Y = [to_number[x] for x in train_R_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using R = 128, D = 6, gamma = 0.1\n",
      "[v_min, v_max] = [-1.171875, 1.3203125]\n",
      "R: 1/128\n",
      "R: 2/128\n",
      "R: 3/128\n",
      "R: 4/128\n",
      "R: 5/128\n",
      "R: 6/128\n",
      "R: 7/128\n",
      "R: 8/128\n",
      "R: 9/128\n",
      "R: 10/128\n",
      "R: 11/128\n",
      "R: 12/128\n",
      "R: 13/128\n",
      "R: 14/128\n",
      "R: 15/128\n",
      "R: 16/128\n",
      "R: 17/128\n",
      "R: 18/128\n",
      "R: 19/128\n",
      "R: 20/128\n",
      "R: 21/128\n",
      "R: 22/128\n",
      "R: 23/128\n",
      "R: 24/128\n",
      "R: 25/128\n",
      "R: 26/128\n",
      "R: 27/128\n",
      "R: 28/128\n",
      "R: 29/128\n",
      "R: 30/128\n",
      "R: 31/128\n",
      "R: 32/128\n",
      "R: 33/128\n",
      "R: 34/128\n",
      "R: 35/128\n",
      "R: 36/128\n",
      "R: 37/128\n",
      "R: 38/128\n",
      "R: 39/128\n",
      "R: 40/128\n",
      "R: 41/128\n",
      "R: 42/128\n",
      "R: 43/128\n",
      "R: 44/128\n",
      "R: 45/128\n",
      "R: 46/128\n",
      "R: 47/128\n",
      "R: 48/128\n",
      "R: 49/128\n",
      "R: 50/128\n",
      "R: 51/128\n",
      "R: 52/128\n",
      "R: 53/128\n",
      "R: 54/128\n",
      "R: 55/128\n",
      "R: 56/128\n",
      "R: 57/128\n",
      "R: 58/128\n",
      "R: 59/128\n",
      "R: 60/128\n",
      "R: 61/128\n",
      "R: 62/128\n",
      "R: 63/128\n",
      "R: 64/128\n",
      "R: 65/128\n",
      "R: 66/128\n",
      "R: 67/128\n",
      "R: 68/128\n",
      "R: 69/128\n",
      "R: 70/128\n",
      "R: 71/128\n",
      "R: 72/128\n",
      "R: 73/128\n",
      "R: 74/128\n",
      "R: 75/128\n",
      "R: 76/128\n",
      "R: 77/128\n",
      "R: 78/128\n",
      "R: 79/128\n",
      "R: 80/128\n",
      "R: 81/128\n",
      "R: 82/128\n",
      "R: 83/128\n",
      "R: 84/128\n",
      "R: 85/128\n",
      "R: 86/128\n",
      "R: 87/128\n",
      "R: 88/128\n",
      "R: 89/128\n",
      "R: 90/128\n",
      "R: 91/128\n",
      "R: 92/128\n",
      "R: 93/128\n",
      "R: 94/128\n",
      "R: 95/128\n",
      "R: 96/128\n",
      "R: 97/128\n",
      "R: 98/128\n",
      "R: 99/128\n",
      "R: 100/128\n",
      "R: 101/128\n",
      "R: 102/128\n",
      "R: 103/128\n",
      "R: 104/128\n",
      "R: 105/128\n",
      "R: 106/128\n",
      "R: 107/128\n",
      "R: 108/128\n",
      "R: 109/128\n",
      "R: 110/128\n",
      "R: 111/128\n",
      "R: 112/128\n",
      "R: 113/128\n",
      "R: 114/128\n",
      "R: 115/128\n",
      "R: 116/128\n",
      "R: 117/128\n",
      "R: 118/128\n",
      "R: 119/128\n",
      "R: 120/128\n",
      "R: 121/128\n",
      "R: 122/128\n",
      "R: 123/128\n",
      "R: 124/128\n",
      "R: 125/128\n",
      "R: 126/128\n",
      "R: 127/128\n",
      "R: 128/128\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-01ed9a2ad5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlambda_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlamda_inverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0maccuracy_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CV = 10\n",
    "R_values = [ 128, 256 ]\n",
    "D_values = [ 6 ]\n",
    "gamma_values = [ 0.1 ]\n",
    "N = len(train_R_X)\n",
    "\n",
    "for R_val in R_values:\n",
    "    for D_val in D_values:\n",
    "        for gamma_val in gamma_values: \n",
    "            print(\"Using R = \" + str(R_val) + \", D = \" + str(D_val) + \", gamma = \" + str(gamma_val))\n",
    "            embedding_points = calculate_WME(train_R_X,D_val,R_val,wvmodel,gamma_val)\n",
    "            best_accuracy = -1\n",
    "            best_lambda = 'boh'\n",
    "            best_std = 0\n",
    "            lamda_inverse = ['1e2', '3e2', '5e2', '8e2', '1e3', '3e3', '5e3', '8e3', '1e4', '3e4', '5e4', '8e4', '1e5' ,'3e5' ,'8e5' ,'1e6', '3e6', '7e6' ,'1e7' ,'5e7' ,'3e8' ,'1e9']\n",
    "            for lambda_val in lamda_inverse:\n",
    "                accuracy_vector = []\n",
    "                kf = KFold(n_splits=10)\n",
    "                kf.get_n_splits(embedding_points)\n",
    "                for train_index, test_index in kf.split(embedding_points):\n",
    "                    X_train = np.array(embedding_points)[train_index]\n",
    "                    X_test = np.array(embedding_points)[test_index]\n",
    "                    \n",
    "                    Y_train = np.array(int_train_R_Y)[train_index]\n",
    "                    Y_test = np.array(int_train_R_Y)[test_index]\n",
    "                    \n",
    "                    prob = problem(Y_train,X_train)\n",
    "                    param = parameter('-s 2 -e 0.0001 -q -c ' + lambda_val)\n",
    "                    m = train(prob,param)\n",
    "                    p_label, p_acc, p= predict(Y_test, X_test, m)\n",
    "                    ACC, MSE, SCC = evaluations(Y_test, p_label)\n",
    "                    accuracy_vector.append(ACC)\n",
    "                avg_ACC = np.mean(accuracy_vector)\n",
    "                std_ACC = np.std(accuracy_vector)\n",
    "                if(avg_ACC > best_accuracy):\n",
    "                    \n",
    "                    best_accuracy = avg_ACC\n",
    "                    best_lambda = lambda_val\n",
    "                    best_std = std_ACC\n",
    "            \n",
    "            print(\"R = \" + str(R_val) + \" D = \" + str(D_val) + \" gamma = \" + str(gamma_val) + \" accuracy = \" + str(best_accuracy))\n",
    "            #We need to save datas here\n",
    "            f = open('increasingR_newsgroup_WME-R'+str(R_val)+\"-D\"+str(D_val)+\"-gamma\"+str(gamma_val)+ '.pckl','wb')\n",
    "            obj = [train_R_X,D_val,R_val,gamma_val,embedding_points,best_accuracy, best_std,best_lambda]\n",
    "            pickle.dump(obj, f) \n",
    "            f.close()\n",
    "\n",
    "            \n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81649658092772603"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86794326841\n"
     ]
    }
   ],
   "source": [
    "print(best_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
